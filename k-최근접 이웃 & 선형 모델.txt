Chapter2 : 지도학습 
지도학습 – 분류 : 가능성 있는 여러 클래스 레이블 중 하나를 예측, 회귀 : 연속적인 숫자를 예측
과대적합 : 너무 복잡한 모델을 만드는 것, 과소적합 : 너무 간단한 모델이 선택    
2.3.2 k-최근접 이웃                                                                                                                                                                                                                                                                                                                                                                                                                                                              
K-최근접 이웃 : 새로운 데이터 포인트에 대해 예측할 때 훈련 데이터셋에서 가장 가까운 훈련 데이터 포인트를 최근접 이웃으로 찾아 예측으로 사용
데이터 포인트 3개 추가 -> 추가한 각 데이터 포인트에서 가장 가까운 훈련 데이터 포인트 연결
임의의 k개를 선택 시 -> 테스트 포인트 하나에 대해 클래스 0에 속한 이웃이 몇 개 인지, 클래스 1에 속한 이웃이 몇 개 인지 센 후 이웃이 더 많은 클래스를 레이블로 지정
Scikit-learn(사이킷런) -> 훈련 세트와 테스트 세트로 나눈다, KNeighborsClassifier를 import 하고 객체를 만든다. 데이터를 저장(학습) – clf.fit(X_train, y_train), predict 메서드를 호출하여 예측, 일반화가 잘 되었는지 score 메서드로 평가

KNeighborsClassifier 분석
각 데이터 포인트가 속한 클래스에 따라 xy평면에 색을 칠한다. 
클래스 0, 클래스1로 지정한 영역으로 나뉜다. -> 결정경계
이웃 수를 늘릴수록 결정 경계는 더 부드러워진다 -> 더 단순
유방암 데이터셋 - 훈련세트와 테스트 세트로 나눈다. 이웃의 수를 달리하여 훈련세트, 테스트세트 성능 평가 -> 이웃의 수가 늘어나면 모델은 단순, 정확도 감소
정확도가 가장 좋을 때 -> 이웃의 수가 중간 정도인 여섯 개 사용시

k-최근접 이웃 회귀
wave 데이터셋을 이용해서 이웃이 하나인 최근접 이웃 사용 - x축에 세 개의 테스트 데이터를 흐린 별 모양으로 표시 – 가장 가까운 이웃의 타깃값
이웃을 둘 이상 – 이웃간의 평균이 예측

Scikit-learn -> import KNeightborsRegressor, wave 데이터셋을 훈련 세트와 테스트 세트로 나눈다, 이웃의 수를 3으로 하여 모델의 객체를 만든다, 모델 학습(fit), 예측, 평가(score – 회귀일때 R^2(R제곱) 반환) 

KNeighborsRegressor 분석
-3과 3 사이 1000개의 데이터 포인트를 만든다, 1, 3, 9 이웃을 사용한 예측
이웃이 1개 – 훈련 세트의 각 데이터 포인트가 예측에 주는 영향이 커서 예측값이 훈련 데이터 포인트를 모두 지나감. 이웃을 많이 사용할수록 더 안정된 예측을 얻는다.

KNeighbors 분류기에 중요한 매개변수 – 데이터 포인트 사이의 거리를 재는 방법, 이웃 수 
k-NN의 장점은 이해하기 쉽다. 단점은 예측이 느리고 많은 특성을 처리하는 능력이 부족

2.3.3 선형 모델
k-NN의 단점이 없는 알고리즘 – 선형모델
선형모델 : 입력 특성에 대한 선형 함수를 만들어 예측을 수행함.
회귀의 선형 모델을 위한 일반화된 예측 함수 w[0]*x[0]+w[1]*x[1]+…+w[p]*x[p]+b
x[0]부터 x[p]까지는 하나의 데이터 포인트에 대한 특성(특성의 개수는 p+1), w와 b는 모델이 학습할 파라미터, y는 모델이 만들어낸 예측값, 직선의 방정식과 유사
회귀를 위한 선형 모델은 특성 1개 – 직선, 특성 2개 – 평면, 그 이상 – 초평면

선형회귀(최소제곱법) : 예측과 훈련 세트에 있는 타깃 y사이의 평균제곱오차를 최소화하는 파라미터 w와 b를 찾는다. 평균제곱오차 -> 예측값과 타깃값의 차이를 제곱하여 더한 후 샘플의 개수로 나눈 것
Import LinearRegreesion, 기울기 파라미터(w)는 lr객체의 coef_속성에 저장, 절편 파라미터(b)는 intercept_속성에 저장
R제곱이 0.66 훈련세트 & 테스트 세트 점수가 매우 비슷 – 과소적합
보스턴 주택가격 데이터 셋인 경우 – 샘풀 : 506개, 특성 : 105개
결과 -> 훈련세트는 예측이 정확, 테스트 세트 R제곱값이 매우 낮다. – 복잡도를 제어하기 어렵
훈련 데이터와 테스트 데이터 사이의 성능 차이 -> 과대적합

릿지 회귀 : 가중치(w)의 값을 가능한 작게 만들기.(w의 모든 원소가 0에 가깝게 만들기-기울기 작게) ->이러한 제약을 규제라고 한다. 
규제란 과대적합(복잡, 모델이 훈련 세트의 각 샘플에 너무 가깝게 맞춰져서 새로운 데이터에 일반화되기 어려울 때) 이 되지 않도록 모델을 강제로 제한하는 것이다.
릿지 회귀에서 사용하는 규제 : L2규제
보스턴 주택가격 데이터 셋을 릿지 회귀에 적용시키면 선형 회귀보다 훈련 세트 점수는 더 낮지만 테스트 세트 점수는 더 높다. (선형 모델일 경우는 과대적합이지만, 릿지 회귀를 적용시키면 과대적합이 적어진다.)  모델의 복잡도가 낮으면 더 일반화된 모델
Alpha 매개변수로 훈련 세트의 성능 대비 모델을 얼마나 단순화할지 지정 가능
alpha값을 높이면 계수를 0에 가깝게 만들고 일반화에 도움을 준다.
높은 alpha 값은 coef_(coefficient 계수)의 절대값 크기(기울기)가 작다. 
릿지, 선형 회귀 모두 훈련 세트의 점수가 테스트 세트 점수보다 높다.
테스트 데이터에서는 릿지의 점수가 높다. 두 모델의 성능은 데이터가 많아질수록 좋아진다. 
	데이터를 충분히 주면 규제 항은 덜 중요해져 릿지 회귀와 선형 회귀의 성능은 같아진다.

라쏘 : 릿지 회귀와 같이 계수를 0에 가깝게 만든다. 라쏘는 L1 규제
L1 규제의 결과로 모델에서 완전히 제외되는 특성(어떤 계수는 정말로 0이 된다.)이 생긴다.
라쏘 – 훈련세트, 테스트 세트 모두 결과가 안좋다. (과소적합)
과소적합을 줄이기 위해 alpha 값을 줄이고, max_iter(반복 실행하는 최대 횟수)의 기본값을 늘린다.
alpha값을 낮추면 복잡도 증가, 성능이 좋아진다. 
특성이 많고 일부분만 중요, 분석 쉽다. – 라쏘

분류용 선형 모델 : 예측한 값을 임계치 0 과 비교 / 계산한 값이 0보다 작으면 클래스 : -1, 계산한 값이 0보다 크면 클래스 : +1
결정 경계가 입력의 선형 함수이다.(이진 선형 분류기는 선, 평면, 초평면을 사용해서 두 개의 클래스를 구분하는 분류기)
두개의 선형 분류 알고리즘 – linear_model.LogisticRegression에 구현된 로지스틱 회귀, svm.LinearSVC에 구현된 선형 서포트 벡터 머신이다. 
결정 경계가 직선으로 표현되고, 위쪽은 클래스1, 아래쪽은 클래스 0으로 나뉜다.
두 모델은 L2규제 사용
LogisticRegression, LinearSVC에서 규제 강도를 결정하는 매개변수 : C
C의 값 높 – 규제 감소(훈련 세트에 가능한 최대로 맞추려 함) (개개의 데이터 포인트를 정확히 분류하려고 노력함.)
C의 값 낮 – 계수 벡터(w)가 0에 가까워진다. (데이터 포인트 중 다수에 맞추려 함.)
C값이 커질수록 결정 경계가 기울면서 거의 모든 데이터 포인트를 올바르게 분류함.

유방암 데이터 셋을 이용해 LogisticRegression을 분석 – 95%의 정확도
C=100 으로 증가 – 훈련 세트, 테스트 세트 점수 정확도 증가 
C=0.01로 감소 – 훈련 세트와 테스트 세트의 정확도는 낮아짐. 
L2규제 모델 사용, 규제를 강하게 할수록 계수들을 0에 더 가깝게 만든다.
L1규제 : 일부 특성만 사용, 더 이해하기 쉽다.
L2규제 : 전체 특성을 모두 사용

다중 클래스 분류용 선형 모델
많은 선형 모델은 이진 분류만을 지원한다. – 다중 클래스를 지원하지 않는다.
이진 분류 알고리즘을 다중 클래스 분류 알고리즘으로 확장하는 방법 : 일대다 방법
일대다 방식은 각 클래스를 다른 모든 클래스와 구분하도록 이진 분류 모델을 학습시킨다.
결국 각 클래스의 수만큼 이진 분류 모델이 만들어짐.
예측 시, 모든 이진 분류기가 작동하여 가장 높은 점수를 내는 분류기의 클래스를 예측값으로 선택. 클래스별 이진 분류기를 만들면 각 클래스가 계수 벡터(w)와 절편(b)을 하나씩 갖게 된다.
w[0]*x[0]+w[1]*x[1]+…+w[p]*x[p]+b 공식의 결과값이 가장 높은 클래스가 해당 데이터의 클래스 레이블로 할당된다.

세 개의 클래스를 가진 간단한 데이터셋에 일대다 방식 적용 - 그림 2-19 세 개의 클래스를 가진 2차원 데이터셋
coef_배열의 크기는 (3, 2)이다. Coef_의 행은 세 개의 클래스에 각각 대응하는 계수 벡터를 담고 있고, 열은 각 특성에 따른 계수 값을 가지고 있다.
Intercept_는 각 클래스의 절편을 담은 1차원 벡터이다.
훈련 데이터의 클래스 0에 속한 모든 포인트는 클래스 0지역에, 클래스 0에 속한 포인트는 클래스 2를 구분하는 직선 위, 클래스 1을 구분하는 직선 왼쪽에 있다.
이 영역의 최종 분류기는 클래스 0으로 분류한다.
중앙의 삼각형 영역의 데이터 포인트는 분류 공식의 결과가 가장 높은 클래스로 분류된다.
즉, 가장 가까운 직선의 클래스가 된다.

alpha값이 클수록 C값이 작을수록 모델이 단순해진다.
로그 스케일로 C와 alpha를 정하고, L1규제를 사용할지 L2 규제를 사용할지 정한다.
선형 모델의 장점은 학습 속도가 빠르고 예측도 빠르다, 쉽게 이해가 가능하다.
선형 모델은 샘플에 비해 특성이 많을 때 잘 작동한다.

